
"""
Python многое делает за нас. Мы привыкли не заботиться об управлении памятью и о написании соответствующего кода.
Пусть эти процессы и скрыты, но без их понимания трудно подготовить производительный код для высоконагруженных задач.
"""
# видео по теме В. Синицын - Python: управление памятью
# https://www.youtube.com/watch?v=ZxvwZ4fX_qE&t=1059s&ab_channel=%D0%92%D0%B8%D0%B4%D0%B5%D0%BE%D1%81%D0%BA%D0%BE%D0%BD%D1%84%D0%B5%D1%80%D0%B5%D0%BD%D1%86%D0%B8%D0%B9IT-People

"""
Инфо о текущем распределении памяти в аренах, пулах и блоках можно посмотреть, запустив функцию sys._debugmallocstats().
# sys._debugmallocstats():
"""
"""
 Функция sys.getrefcount позволит узнать число ссылок на объект
(но она накинет единицу, т.к. ее аргумент — тоже ссылка на тестируемый объект):
Когда счетчик уменьшается до нуля, происходит вызов аллокатора для  освобождения соответствующих блоков памяти.
# >>> foo = []
# >>> import sys
# >>> sys.getrefcount(foo)
# 2
"""
""" 
Однако счетчик ссылок неспособен отследить ситуации с циклическими ссылками. К примеру, возможна ситуация,
 когда два объекта ссылаются друг на друга, но оба уже не используются программой. Для борьбы с такими зависимостями 
 используется сборщик мусора (garbage collector). Если счетчик ссылок является свойством объекта, 
 то сборщик мусора — механизм, который запускается на основе эвристик.
  Задача этих эвристик — снизить частоту и объем очищаемых данных.
   Основная стратегия заключается в разделении объектов на поколения: чем больше сборок мусора пережил объект, 
   тем он значимее для выполнения работы программы. 
Подсчет ссылок не может быть отключен, а gc — может. В некоторых случаях полезно 
отключить автоматическую сборку gc.disable() и вызывать его вручную gc.collect().
# gc — Garbage Collector interface
# https://docs.python.org/3/library/gc.html
"""

# Давайте напишем функцию, показывающую размер объектов (рекурсивно, если нужно):
# https://habr.com/ru/company/mailru/blog/336156/

import sys
def show_sizeof(x, level=0):
    print ("\t" * level, x.__class__, sys.getsizeof(x), x)
    if hasattr(x, '__iter__'):
        if hasattr(x, 'items'):
            for xx in x.items():
                show_sizeof(xx, level + 1)
        else:
            for xx in x:
                show_sizeof(xx, level + 1)


show_sizeof(3)
show_sizeof(2**63)

""" И что нам всё это даёт? Когда ваш проект не начнёт разрастаться, тогда вам придётся очень аккуратно следить
 за количеством создаваемых объектов, чтобы ограничить объём потребляемой приложением памяти. 
 Для настоящих приложений это проблема. Чтобы разработать действительно хорошую стратегию управления памятью,
  нам нужно следить не только за размером новых объектов, но и за количеством и порядком их создания. 
  Для Python-программ это очень важно. """

"""
если элемент (размером x) удалён из памяти (стёрта ссылка на него), то занимавшийся им объём не возвращается в
пул глобальной памяти Python (в том числе и в систему), а помечается свободным и добавляется к списку
свободных элементов размером x. Занимаемый мёртвым объектом объём может быть использован вновь,
если понадобится другой объект подходящего размера. А если подходящего мёртвого объекта нет, то создаётся новый.
Если память с маленькими объектами никогда не освобождается, то мы приходим к неизбежному выводу,
что эти списки с маленькими объектами могут только расти, они никогда не уменьшаются, а значит,
в каждый момент времени в памяти вашего приложения преобладают размещённые в ней многочисленные маленькие объекты.
"""

# Докажем это утверждение с помощью memory_profiler, модуля для Python.
# Он добавляет декоратор @profile, позволяющий отслеживать какое-то конкретное применение памяти.
# To install through pip:
# pip install -U memory_profiler


import copy
from memory_profiler import profile

@profile(precision=3)
def function():
    x = list(range(1000000))  # allocate a big list
    y = copy.deepcopy(x)
    del x
    return y

if __name__ == "__main__":
    function()


"""  результат в Python 3.9.5 на 64-битном компьютере и Windows 10 она выводит:

Line #    Mem usage    Increment  Occurences   Line Contents
============================================================
    63  18.9297 MiB  18.9297 MiB           1   @profile(precision=4)
    64                                         def function():
    65  57.5469 MiB  38.6172 MiB           1       x = list(range(1000000))  # allocate a big list
    66  66.1211 MiB   8.5742 MiB           1       y = copy.deepcopy(x)
    67  58.4883 MiB  -7.6328 MiB           1       del x
    68  58.4883 MiB   0.0000 MiB           1       return y


Программа создаёт n = 1 000 000 целых чисел (n × 24 байта = ~23 Мб) и дополнительный список ссылок 
(n × 8 байтов = ~7,6 Мб), и в сумме получаем ~31 Мб. copy.deepcopy копирует оба списка, и копии занимают ~50 Мб.
Любопытно, что del x удаляет x, но потребление памяти снижается лишь на 7,63 Мб! Причина в том, что del удаляет только 
список ссылок, а реальные целочисленные значения остаются в куче и приводят к избыточному потреблению в ~23 Мб.
В этом примере в сумме занято ~73 Мб, что более чем вдвое превышает объём, необходимый для хранения списка,
 весящего ~31 Мб. При потере бдительности порой возникают очень неприятные сюрпризы с точки зрения потребления памяти!
Вы можете получить иные результаты на других платформах и других версиях Python.
"""

"""
Заключение
управление памятью в Python обрабатывается автоматически с помощью стратегий подсчета ссылок и сбора мусора.
Без garbage collection реализация успешного механизма управления памятью в Python невозможна.
CPython использует много памяти для своих объектов. Он использует различные приемы и оптимизации для управления памятью.
 Отслеживая использование памяти вашим объектом и зная модель управления памятью,
  вы можете значительно уменьшить объем памяти вашей программы.
Сохранение и освобождение блоков памяти требует времени и вычислительных ресурсов. Чем меньше блоков задействовано,
 тем выше скорость работы программы. Позволим себе дать несколько советов, касающихся экономной работы с памятью:

1. Обращайте внимание на работу с неизменяемыми объектами.
    К примеру, вместо использования оператора + для соединения строк используйте методы .join(), .format() или f-строки.
2. Избегайте вложенных циклов. Создание сложных вложенных циклов приводит к генерации чрезмерно большого количества
    объектов, занимающих значительную часть виртуальной памяти. Большинство задач, решаемых с помощью вложенных циклов, 
    разрешимы методами модуля itertools.
3. Используйте кэширование. Если вы знаете, что функция или класс используют или генерируют набор однотипных объектов,
    применяйте кэширование. Часто для этого достаточно добавить всего лишь один декоратор из библиотеки functools.
4. Профилируйте код. Если программа начинает «тормозить», то профилирование — самый быстрый способ найти корень всех зол

На ранних этапах работы с кодом Python можно вполне обойтись стандартными средствами Python.
 Но по мере разрастания кодовой базы и приближения к коммерческому использованию кода 
 !!!производительность становится одним из ключевых факторов.
  Многие задачи, связанные с производительностью Python, лежат в области понимания устройства инструмента
   и конкретной реализации языка. Не пожалейте времени на изучение исходного кода CPython,
    находящегося в свободном доступе в репозитории на GitHub.
"""

